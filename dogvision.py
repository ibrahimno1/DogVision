# -*- coding: utf-8 -*-
"""DogVision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WraoWjLjIuNZUFpJ2zIbTnsbKNY0FV3Z
"""

# !unzip "/content/drive/MyDrive/Dog Vision/dog-breed-identification.zip" -d "drive/MyDrive/Dog Vision/"

"""# Dog End-to-End Multiclass Dog Breed Classification

This notebook build an end to end multiclass image classifier using TensorFlow 2.0 and TensorFlow hub.

## Problem

Identifying breed of a dog  given an image of a dog

If I take a photo of a dog, I want to use this model to determine what type of dog it is

## Data

The data we're using is from Kaggle Dog Breed identification competition

## Evaluation

The evaluation is a file with prediction probabilities for   each dog breed of each test image

## Features

Information about the data:
* We're dealing with images (unstructured data) so it's probably best we use deep learning/transfer learning.

* There are 120 breeds of dogs (this means there are 120 different classes)
* There are 10,000+ images in the training set (these images have labels)
* There are 10,000+ images in the test set (these images have no label - we will be predicting the labels)

### Get our workspace Ready

* Import TensorFlow 2.x
* Import Tensorflow hub
* Make sure we're using GPU
"""

# import tensorflow into colab

import tensorflow as tf
print('TF version:', tf.__version__)

import tensorflow_hub as hub
print('Hub version:', hub.__version__)

# Check for GPU
print('GPU', 'available (yesss)' if tf.config.list_physical_devices('GPU') else 'not available')

"""## Getting our data ready

Turing our images into numerical representation.

Lets start by accessing our data and checking out our labels

"""

# Checkout labels for our data

import pandas as pd
labels_csv = pd.read_csv('drive/MyDrive/Dog Vision/labels.csv')
print(labels_csv.describe())
print(labels_csv.head())

labels_csv.head()

# How manu images are there of each breed?

labels_csv['breed'].value_counts().plot.bar(figsize=(20,10))

#View the image

from IPython.display import Image
Image('drive/MyDrive/Dog Vision/train/032c14b2df4193004913b01ab48f87c6.jpg')

"""### Getting images and their labels

Lets get a list all of our image and file pathnames
"""

# Create pathname with Image IDs
filenames = ["drive/MyDrive/Dog Vision/train/" + fname + '.jpg' for fname in labels_csv['id']]
filenames[136]

# Check if number of files in the train folder is the same len as filenames
import os

if len(os.listdir('drive/MyDrive/Dog Vision/train/')) == len(filenames):
  print('Number of files in train folder is equal to number of filenames')
else:
  print('Number of files in train folder is not equal to number of filenames')

#Test

Image(filenames[9000])

labels_csv['breed'][38]

"""Since we've now got our training image filepaths in a list, let's prepare our labels"""

import numpy as np
labels = np.array(labels_csv['breed'])
labels

unique_breeds[19]

# see if number of labels matches the number of filenames
import os

if len(os.listdir('drive/MyDrive/Dog Vision/train/')) == len(labels):
  print('Number of Labels in train folder is equal to number of filenames')
else:
  print('Number of labels in train folder is not equal to number of filenames')

# Find the unieuq label values

unique_breeds = np.unique(labels)
len(unique_breeds)

# Turn a single label into an array of booleans
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

# Example: Turning boolean array into integers

print(labels[0]) # label
print(np.where(unique_breeds == labels[0])) # index where label accounts
print(boolean_labels[0].argmax()) # index where labels occurs in boolean array
print(boolean_labels[0].astype(int)) # there will be a 1 where the sample label occurs

boolean_labels[:2]

"""### Creating our own validation set

"""

# Setup x and y

x= filenames
y= boolean_labels

"""We're going to start off experimenting with ~1000 images and increase as needed"""

# set number of images to use for experimenting

NUM_IMAGES = 1000 #@param {type:"slider", min:1000, max:10000, step:1000}

# Let's split our data into train and validation sets

from sklearn.model_selection import train_test_split

#Split them into training and validation of total size NUM_IMAGES
x_train, x_val, y_train, y_val = train_test_split(x[:NUM_IMAGES],
                                                    y[:NUM_IMAGES],
                                                    test_size=0.2,
                                                    random_state=42)

len(x_train), len(x_val), len(y_train), len(y_val)

x_train[:2], y_train[:2]

"""## Preprocessing Images (Turning images into Tensors)

To preprocess our images into tensors we're going to write a function which does a few things:
1. Take an image filepath as input
2. Use TensorFlow to read the file and save it to a variable, `image`
3. Turn our `image` into Tensors
4. Normalize our image (convert color channel values from 0-255 to 0-1)
5. Resize the `image` to be a shape of (224,224)
6. Return the modified `image`
"""

# Convert image to Numpy array

from matplotlib.pyplot import imread
image = imread(filenames[42])
image.shape

tf.constant(image)

"""1. Take an image filepath as input
2. Use TensorFlow to read the file and save it to a variable, image
3. Turn our image into Tensors
4. Resize the image to be a shape of (224,224)
5. Return the modified image
"""

# Define Image Size
IMG_SIZE  = 224

#Create a function for preprocessing image

def process_images(image_path, IMG_SIZE=224):
  '''
  Takes an image file path and turns it into a Tensor.
  '''
  # Rand in an image file
  image = tf.io.read_file(image_path)

  #Turn the jpeg image into Numerical Tensor with 3 colours channels (red, green, blue)
  image = tf.image.decode_jpeg(image, channels=3)

  #Convert the colour channel values from 0-255 to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)

  #Resize the image to our desired size (224,22)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])

  return image

"""## Turning our data into batches

Let's say you're to process 10,000+ images in one go .... they all might not fit into memory.

That's why we do 32 (this is the batch - can be manually adjusted) images at a time

We need our data in the form of Tensor tuples:
`(image,label)`
"""

# Create a funtion to return a tuple

def get_image_label(image_path, label):
  '''
  Takes an image file path name and the associated label,
  processed the image and return a tuple
  '''
  image = process_images(image_path)
  return image, label

(process_images(x[42]), tf.constant(y[42]))

"""Create a funtion to turn all data into (x&y) batches"""

# Define the batch size , 32 is good start

BATCH_SIZE = 32

# Create a function to turn data into batches

def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  '''
  Creates batches of data out of image (x) and label (y) pairs
  Shuffles the data id it's training data but doesnt if it's validation data.
  Also accepts test data as input (no labels)
  '''

  # if the data is a test dataset, we probably don't have labels
  if test_data:
    print("creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) #ony filepaths
    data_batch = data.map(process_images).batch(BATCH_SIZE)
    return data_batch

  #If the data is a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("creating valid data batch")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), #Filepath
                                               tf.constant(y))) #Label
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print('Creating training data batches....')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))
    #Shuffle pathnames and labels before mapping image processor function
    data = data.shuffle(buffer_size=len(x))
    #Create (image,label) tuples
    data = data.map(get_image_label)
    #Turn tthe training data into batches
    data_batch = data.batch(BATCH_SIZE)

  return data_batch

train_data = create_data_batches(x_train,y_train)
val_data = create_data_batches(x_val, y_val, valid_data=True)

#Check out the different attibutes of our data batches

train_data.element_spec, val_data.element_spec

"""## Visualizing Data Batches

Data Batches are best learned by visualizing, they are hard to comprehend with out visual help
"""

import matplotlib.pyplot as plt

#Create a funtion for viewing images in a data batch

def show_25_image(images, labels):
  '''
  Displays a plot of 25 images and their labels from a data batch
  '''
  plt.figure(figsize=(10,10))
  #Loop through 25 for displaying
  for i in range(25):
    # Create subplts(5 rows, 5 columns)
    ax= plt.subplot(5,5, i+1)
    # Display an image
    plt.imshow(images[i])
    #Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    #Turn the grid lines 0ff
    plt.axis('off')

train_images, train_labels = next(train_data.as_numpy_iterator())
len(train_images), len(train_labels)

val_images, val_labels = next(val_data.as_numpy_iterator())
len(val_images), len(val_labels)

# Now visualize the data in a training batch

show_25_image(train_images,train_labels)

show_25_image(val_images,val_labels)

"""## Building a model

* The input shape (image shape in the form of tensors)
* The output shape (image label in the form of tensors)
* The URL of the model we want to use
"""

# Setup input shape to the model

INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE,3] # Batch, Height, Width, colour channels
OUTPUT_SHAPE = len(unique_breeds)

# Setup model URL from TensorFLow hub
MODEL_URL = "https://www.kaggle.com/models/google/mobilenet-v2/tensorFlow2/130-224-classification"

"""Now we've got our inputs, outputs and model. Putting them together into a Keras deep learning model!

Funtion requirments:
* Accepts Input shape, Output shape, and the model URL
* Defines the layers in a Keras model in sequential fashion(linear step by step process)
* Compiles the model (evaluated and improved)
* Builds the model(input shape)
* Returns the model

All of these steps can be found on tensorflow documentation
"""

# Create a function which builds a Keras model

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print('Building model with: ',model_url )

  # Setup the model layers
  model = tf.keras.Sequential([hub.KerasLayer("https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/130-224-classification/2"),
                               tf.keras.layers.Dense(output_shape,activation='softmax')
                               ])
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=['accuracy']
  )

  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

outputs = np.ones(shape=(1,1,1280))
outputs

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import datetime

def create_tensorboard_callback():
  log_dir = os.path.join("drive/MyDrive/Dog Vision/logs",
                         datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(log_dir)

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
                                                  patience=3)

"""Training a model of 1000"""

NUM_EPOCHS = 100 #@param {type:"slider", min:10, max:100, step:10}

"""Creating a funtion to training the model
* Create a model using `create_model`
* Set up tensor callback
* call the fit funtion with training/valid data and NUM_EPOCHS
* Return the model
"""

def train_model():
  '''
  Trains a given model and returns the trained version
  '''
  #Create a model
  model = create_model()

  #Create new Tensorboard session
  tensorboard = create_tensorboard_callback()

  # Fit the model to the dataq passing it the callbacks
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks= [tensorboard, early_stopping])

  return model

# Fit the model to the data

model = train_model()

"""### Checking the TensorBoard Logs


Tensor board magic funtion (`%tensorboard), will access the logs directory and visualize the contents


"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Dog\ Vision/logs

"""## Making and evaluating predictions using a trained model"""

# Make predictions on the validation data (not used to train on)

predictions = model.predict(val_data, verbose=1)
predictions

predictions.shape

# First predictioni
index= 81
print(predictions[index])
print(f'Max Value (probability): {np.max(predictions[index])}')
print(f'Sum : {np.sum(predictions[index])}')
print(f'Max Index: {np.argmax(predictions[index])}')
print(f'Predictions Label: {unique_breeds[np.argmax(predictions[index])]}')

"""Now we want to do it at scale
also see the image the prediction is being made on
"""

# Turn prediction probabilities into their respective label
def get_pred_label(prediction_probabilities):
  '''
  Turns an array of prediction probabilities into a label
  '''
  return unique_breeds[np.argmax(prediction_probabilities)]

pred_label = get_pred_label(predictions[81])
pred_label

"""Validation is currently in a batch dataset, we have unbatchify it, then cmake predictions on the validation images, then compare to the prediction value to actual"""

# Create a funtion to unbatch dataset



#Loop through unbatches data
def unbatch(data):
  '''
  Takes a batch data if image and label and returns seperate arrays of images and lables
  '''
  images = []
  labels = []
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images, labels

#Unbatify the validation data

val_images, val_labels = unbatch(val_data)
val_images[0], val_labels[0]

"""We have
* Prediction labels
* Validation Labels
* Validation Images

Make funtions to visualize the top data
* Array of prediction probbilities, an array of truth labels, array of images and integers
* Convert the prediction probabilities to a predicted label
* Plot the predicted label, it predicted porbability and the truth label and the target image
"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  '''
  View the prediction, ground truth
  '''
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # Get the pred label
  pred_label = get_pred_label(pred_prob)

  #Plot image and remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  #Change the colour of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = 'green'
  else:
    color = 'red'

  #Change plt title to be predicted, probability of prediction and truth label
  plt.title('{} {:2.0f}% {}'.format(pred_label,np.max(pred_prob)*100,true_label),color=color)

plot_pred(prediction_probabilities=predictions,labels=val_labels, images=val_images, n=76)

"""Visualize the top 10 predictions:
* Take an input og prediction probabilites array and a ground truth array and an integer
* Find the prediction using `get_pred_label()`
* Find the top 10:
  * Prediction probabilities indexes
  * Prediction probabilities values
  * Prediction labels
* PLot the top 10 prediction probability values and labels, coloring the true label green
"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  '''
  Plot the top 10 highest prediction confidences along with the truth label for sample n
  '''

  pred_prob, true_label = prediction_probabilities[n], labels[n]

  #Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 predictions labels
  top_10_pred_lables = unique_breeds[top_10_pred_indexes]

  #Setup Plot
  top_plot = plt.bar(np.arange(len(top_10_pred_lables)),
                      top_10_pred_values,
                      color='grey')
  plt.xticks(np.arange(len(top_10_pred_lables)), labels=top_10_pred_lables, rotation='vertical')

  #Change color of trye label

  if np.isin(true_label, top_10_pred_lables):
    top_plot[np.argmax(top_10_pred_lables == true_label)].set_color('green')
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=100)

predictions[0].argsort()[-10:][::-1]

predictions[0].max()

"""Check out a few rather than 1"""

i_multiplier = 135
num_rows= 3
num_cols = 2
num_images = num_rows * num_cols

plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities= predictions, labels=val_labels, images=val_images, n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities= predictions, labels=val_labels, n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

unique_breeds

"""## saving and reloading a trained model

"""

# Create a funtion to a save a model

def save_model(model, suffix=None):
  '''
  Save a given model in a model directory and appends a suffix (string)
  '''
  #Create a model directory with current time
  modeldir= os.path.join("/drive/MyDrive/Dog Vision/models",datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))

  model_path = modeldir + '-' + suffix + '.h5' #Save format of the model
  print(f'Saving model to:{model_path}...')
  model.save(model_path)
  return model_path

# Create a funtion to load a trained model

def load_model(model_path):
  '''
  Loads a saved model from a specified path
  '''
  print(f'Loading save model from: {model_path}')
  model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer':hub.KerasLayer})

  return model

# Save our model trained on 1000 images

save_model(model, suffix='1000-images-mobilenetv2-Adam')

#Load a trained model

loaded_1000_image_model = load_model('/drive/MyDrive/Dog Vision/models/20240717-000713-1000-images-mobilenetv2-Adam.h5')

# Evaluate the pre-saved model

model.evaluate(val_data)

#Evaluate the loaded model
loaded_1000_image_model.evaluate(val_data)

"""## Training a big dog model on the full data"""

len(x), len(y)

len(x_train)

# Create a data batch with the full data set

full_data = create_data_batches(x,y)

full_data

# Create a model for full model
full_model = create_model()

#Create full model callbacks

full_model_tensorbard = create_tensorboard_callback()

# No validation set when training on all the data, so we can't monitor validation accuracy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy',patience=3)

"""Note: Running the cell will take 30 mins because GPU has to load all of the images into memory"""

#Fit the full model to the full data

full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_early_stopping])

from google.colab import drive
drive.mount('/content/drive')

save_model(full_model, suffix='full-image-set-mobilenetv2-Adam')

loaded_image_model = load_model('/drive/MyDrive/Dog Vision/models/20240718-183245-full-image-set-mobilenetv2-Adam.h5')

"""## Making predictions on the test data
To make predictions on the test data, we will have to convert the test data into batchses

Funtion already created `create_data_batches()`

To Make predictions:
* Get the test image filenames
* Convert the filenames into test data batches using `create_data_batches()` and setting the `test_data` parameter to `True` (does not have labels)
* Make a predictions array by passing the test batches to the `predict()` method

"""

#Load image file names
test_path = 'drive/MyDrive/Dog Vision/test/'
test_filenames= [test_path + fname for fname in os.listdir(test_path)]
test_filenames[:10]

len(test_filenames)

#Create test data batch
test_data = create_data_batches(test_filenames, test_data=True)

test_data

"""Calling predicts on our full model and passing it test will take a lone time (about an hour)"""

# Make Predictions on test data batch using the all train data

test_predictions =loaded_1000_image_model.predict(test_data, verbose=1)

np.savetxt("drive/MyDrive/Dog Vision/preds_array.csv", test_predictions, delimiter=",")

test_predictions = np.loadtxt("drive/MyDrive/Dog Vision/preds_array.csv", delimiter=",")

"""## Making preditions on custom images
* Get the filepaths of our images
* Turn the filepaths into data batches using `create_data_batches()'
* pass the custom images  to our model predict method
* Convedrt the prediction output probabilities to predictions labels
Compare the predicted labels to the custom images
"""

# Get custom image filepaths

# Get custom image filepaths
import os

custom_path= "drive/MyDrive/Dog Vision/gf_dog_2/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path) if not fname.startswith('.')] # Ignore files/directories starting with '.'

custom_image_paths

#Turn custom images into batch datasets

custom_data= create_data_batches(custom_image_paths, test_data=True)
custom_data

#Make Predictions on the custom data

custom_preds = loaded_image_model.predict(custom_data)

# Get custom image filepaths

custom_path= "drive/MyDrive/Dog Vision/gf_dog_2/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path) if not fname.startswith('.')] # Ignore files/directories starting with '.'

# ... (rest of your code)

#Turn custom images into batch datasets

custom_data= create_data_batches(custom_image_paths, test_data=True)
custom_data

# ... (rest of your code)

#Make Predictions on the custom data

custom_preds = loaded_image_model.predict(custom_data)

custom_preds

custom_preds.shape

#Get Custom Image label

custom_pred_label = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_label

# Get custom images
custom_images = []

for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

custom_images

#Check prediction

plt.figure(figsize=(40,20))
for i, image in enumerate(custom_images):
  plt.subplot(1,10, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_label[i])
  plt.imshow(image)

# prompt: For the images in custom_image I want to show the top 10 probabilties - also output the results in jpg .

import matplotlib.pyplot as plt
import numpy as np
# Iterate through custom images and predictions
for i, (image, pred_probs) in enumerate(zip(custom_images, custom_preds)):
  # Get top 10 prediction probabilities and labels
  top_10_pred_indexes = pred_probs.argsort()[-10:][::-1]
  top_10_pred_values = pred_probs[top_10_pred_indexes]
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Plot top 10 predictions
  plt.figure(figsize=(10, 5))
  plt.bar(np.arange(len(top_10_pred_labels)), top_10_pred_values, color='grey')
  plt.xticks(np.arange(len(top_10_pred_labels)), labels=top_10_pred_labels, rotation='vertical')
  plt.title(f"Top 10 Predictions for Custom Image {i+1}")

  # Save the plot as a JPG file
  plt.savefig(f"drive/MyDrive/Dog Vision/custom_image_predictions_{i+1}.jpg")
  plt.show()

unique_breeds

